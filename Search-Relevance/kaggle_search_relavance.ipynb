{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle:Home Depot Product Search Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "df_test = pd.read_csv('test.csv', encoding='ISO-8859-1')\n",
    "df_desc = pd.read_csv('product_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_desc, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原始文本信息进行三点预处理\n",
    "\n",
    "（1）将单词小写并分为tokens\n",
    "\n",
    "（2）去除停止词和数字\n",
    "\n",
    "（3）使用SnowballStemmer 提取词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "def check(word):\n",
    "    if word in stop:\n",
    "        return False\n",
    "    elif hasNumbers(word):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "#def str_stemmer(s):\n",
    "#    return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "df_all['search_term'].str.lower().str.replace('\"','').str.replace(\"'\", '').str.split()\n",
    "df_all['product_title'].str.lower().str.replace('\"','').str.replace(\"'\", '').str.split()\n",
    "df_all['product_description'].str.lower().str.replace('\"','').str.replace(\"'\", '').str.split()\n",
    "\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:stemmer.stem(item) for item in x if check(item))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:stemmer.stem(item) for item in x if check(item))\n",
    "df_all['product_description'] = df_all['product_description'].map(\n",
    "                                lambda x:stemmer.stem(item) for item in x if check(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 文本特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取三项特征\n",
    "\n",
    "（1）文本距离\n",
    "\n",
    "（2）基于TF-IDF算法的文本余弦相似度\n",
    "\n",
    "（3）基于Word2Ver的向量余弦相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）新建两列'dist_in_title'和'dist_in_desc'\n",
    "\n",
    "是search_term与product_title和product_description的文本距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "df_all['dist_in_title'] = df_all.apply(lambda x:Levenshtein.ratio(x['search_term'],x['product_title']), axis=1)\n",
    "df_all['dist_in_desc'] = df_all.apply(lambda x:Levenshtein.ratio(x['search_term'],x['product_description']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）利用TF-IDF计算文本余弦相似度\n",
    "\n",
    "新建两列'tfidf_cos_sim_in_title'和'tfidf_cos_sim_in_desc'\n",
    "\n",
    "为'search_term'与'product_title'和'product_description'的余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['all_texts'] = df_all['product_title'] + '.' + df_all['product_description'] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(list(tokenize(x, errors='ignore')) for x in df_all['all_texts'].values)\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for x in df_all['all_texts'].values:\n",
    "            yield dictionary.doc2bow(list(tokenize(x, errors='ignore')))\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "def to_tfidf(text):\n",
    "    res = tfidf[dictionary.doc2bow(list(tokenize(text, errors='ignore')))]\n",
    "    return res\n",
    "def cos_sim(text1, text2):\n",
    "    tfidf1 = to_tfidf(text1)\n",
    "    tfidf2 = to_tfidf(text2)\n",
    "    index = MatrixSimilarity([tfidf1],num_features=len(dictionary))\n",
    "    sim = index[tfidf2]\n",
    "   \n",
    "    return float(sim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['tfidf_cos_sim_in_title'] = df_all.apply(lambda x: cos_sim(x['search_term'], x['product_title']), axis=1)\n",
    "df_all['tfidf_cos_sim_in_desc'] = df_all.apply(lambda x: cos_sim(x['search_term'], x['product_description']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）利用Word2Vec计算单词向量的余弦相似度\n",
    "\n",
    "新建两列w2v_cos_sim_in_title和w2v_cos_sim_in_desc\n",
    "\n",
    "为search_term与product_title和product_description所有单词向量的余弦相似度的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize(df_all['all_texts'].values[0])\n",
    "sentences = [tokenizer.tokenize(x) for x in df_all['all_texts'].values]\n",
    "sentences = [y for x in sentences for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "w2v_corpus = [word_tokenize(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(w2v_corpus, size=128, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.vocab\n",
    "def get_vector(text):\n",
    "    res = np.zeros([128])\n",
    "    count = 0\n",
    "    for word in word_tokenize(text):\n",
    "        if word in vocab:\n",
    "            res += model[word]\n",
    "            count +=1\n",
    "    return res/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def w2v_cos_sim(text1, text2):\n",
    "    try:\n",
    "        w2v1 = get_vector(text1)\n",
    "        w2v2 = get_vector(text2)\n",
    "        sim = 1 - spatial.distance.cosine(w2v1, w2v2)\n",
    "        return float(sim)\n",
    "    except:\n",
    "        return float(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_all['w2v_cos_sim_in_title'] = df_all.apply(lambda x: w2v_cos_sim(x['search_term'], x['product_title']), axis=1)\n",
    "df_all['w2v_cos_sim_in_desc'] = df_all.apply(lambda x: w2v_cos_sim(x['search_term'], x['product_description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop(['search_term','product_title','product_description','all_texts'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>dist_in_title</th>\n",
       "      <th>dist_in_desc</th>\n",
       "      <th>tfidf_cos_sim_in_title</th>\n",
       "      <th>tfidf_cos_sim_in_desc</th>\n",
       "      <th>w2v_cos_sim_in_title</th>\n",
       "      <th>w2v_cos_sim_in_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.274539</td>\n",
       "      <td>0.182836</td>\n",
       "      <td>0.465428</td>\n",
       "      <td>0.459099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337834</td>\n",
       "      <td>0.147382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053455</td>\n",
       "      <td>0.347624</td>\n",
       "      <td>0.468898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.048632</td>\n",
       "      <td>0.133577</td>\n",
       "      <td>0.043712</td>\n",
       "      <td>0.558218</td>\n",
       "      <td>0.489915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.397320</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>0.489721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid  relevance  dist_in_title  dist_in_desc  \\\n",
       "0   2       100001       3.00       0.190476      0.030418   \n",
       "1   3       100001       2.50       0.153846      0.022901   \n",
       "2   9       100002       3.00       0.175000      0.017875   \n",
       "3  16       100005       2.33       0.326087      0.048632   \n",
       "4  17       100005       2.67       0.382979      0.054545   \n",
       "\n",
       "   tfidf_cos_sim_in_title  tfidf_cos_sim_in_desc  w2v_cos_sim_in_title  \\\n",
       "0                0.274539               0.182836              0.465428   \n",
       "1                0.000000               0.000000              0.337834   \n",
       "2                0.000000               0.053455              0.347624   \n",
       "3                0.133577               0.043712              0.558218   \n",
       "4                0.397320               0.098485              0.727232   \n",
       "\n",
       "   w2v_cos_sim_in_desc  \n",
       "0             0.459099  \n",
       "1             0.147382  \n",
       "2             0.468898  \n",
       "3             0.489915  \n",
       "4             0.489721  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 分割训练、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_all.loc[df_train.index]\n",
    "df_test = df_all.loc[df_test.index]\n",
    "test_ids = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train.drop(['id','relevance'],axis=1).values\n",
    "X_test = df_test.drop(['id','relevance'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('X_train_V1.out',X_train)\n",
    "np.savetxt('y_train_V1.out',y_train)\n",
    "np.savetxt('X_test_V1.out',X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## step5 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选用RandomForestRegressor，XGBRegressor，Ridge三种模型，\n",
    "\n",
    "超参数使用GridSearch方法调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor  \n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.linear_model import Ridge\n",
    "clfs = [RandomForestRegressor(n_estimators=15, max_depth=6),  \n",
    "        XGBRegressor(max_depth=10, learning_rate=0.3, n_estimators=150, silent=True, \n",
    "                    objective='reg:linear', nthread=-1, gamma=0, eval_metric='rmse',\n",
    "                    max_delta_step=0, subsample=0.85, colsample_bytree=0.7, \n",
    "                    colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, \n",
    "                    seed=0, missing=None),  \n",
    "        Ridge(alpha=12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step6 模型融合并输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用stacking方式模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_stack_train = np.zeros((X_train.shape[0],len(clfs)))  \n",
    "dataset_stack_test = np.zeros((X_test.shape[0],len(clfs)))  \n",
    "for j,clf in enumerate(clfs):  \n",
    "    clf.fit(X_train,y_train)  \n",
    "    stack_y_submission = clf.predict(X_test)  \n",
    "    stack_y_train = clf.predict(X_train)  \n",
    "    dataset_stack_train[:,j] = stack_y_train  \n",
    "    dataset_stack_test[:,j] = stack_y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=30,max_depth=6)  \n",
    "clf.fit(dataset_stack_train,y_train)  \n",
    "predict = clf.predict(dataset_stack_test)  \n",
    "pd.DataFrame({\"id\": test_ids, \"relevance\": predict}).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
